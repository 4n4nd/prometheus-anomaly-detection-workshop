{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prometheus Anomaly Detection\n",
    "This notebook will give an introduction to how we can perform anomaly detection for Prometheus metrics using a machine learning time series forecasting model. Before we dive into the data science work, let's understand the problem.\n",
    "\n",
    "## Understanding the Problem\n",
    "Prometheus is the chosen application to do monitoring across multiple products and platforms. Prometheus metrics are time series data identified by metric name and key/value pairs. With the increased amount of metrics flowing in it is getting harder to see the signals within the noise. The current state of the art is to graph out metrics on dashboards and alert on thresholds. \n",
    "\n",
    "However, we can leverage machine learning algorithms to perform time series forecasting and predict an unusual behavior or pattern in the metrics. The predicted values from the model can be compared with the actual metric values and if they differ from the default threshold values, we can flag it as an **anomaly**. Before we start looking at any machine learning model, let's try to understand the data.\n",
    "\n",
    "## Understanding the Data\n",
    "As we saw in the previous Prometheus API client notebook, Prometheus metrics are time series data identified by:\n",
    "* **metric name**\n",
    "* **labels**\n",
    "\n",
    "Open the Prometheus UI and see for yourself what the metrics look like!\n",
    "\n",
    "Any given combination of labels for the same metric name identifies a particular dimensional instantiation of that metric. Each metric consists of a timestamp and its corresponding value at that timestamp. Prometheus comprises of four metric types:\n",
    "* **Counter**\n",
    "* **Gauge**\n",
    "* **Histogram**\n",
    "* **Summary**\n",
    "\n",
    "You can find more details about these metrics here: https://prometheus.io/docs/concepts/metric_types/. \n",
    "\n",
    "In this notebook we will walk through some of the essential steps required for training a machine learning model to predict anomalies on a sample Prometheus time series metrics data set:\n",
    "\n",
    "## Data Preparation & Data Exploration\n",
    "Bad data or poor quality of data can alter the accuracy of insights or could lead to incorrect insights, which is why data preparation or data cleaning is of utmost importance even though it is one of the time consuming tasks of the data science process. Some of the tasks in our data pre-processing are:\n",
    "\n",
    "* Load sample metric data set (Json)\n",
    "* Convert the raw metric data into suitable pandas dataframe using the Prometheus API client\n",
    "* Compress the timeseries into suitable (1h/ 0.5h/ 0.25/) frequency for training the model\n",
    "\n",
    "## Model Data\n",
    "This is when the real fun starts. Machine learning algorithms can help you go a step further into getting insights and predicting future trends. Once again, before reaching this stage, bear in mind that the scrubbing and exploring stage are equally crucial to building useful models. So take your time on those stages instead of jumping right to this process. During this stage we will:\n",
    "* Train a ML model\n",
    "* Forecast the predictions on a test data set\n",
    "* Evaluate the predictions for detecting anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's install the required packages/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prometheus-api-client in /home/hemaveeradhi/.local/lib/python3.7/site-packages (0.0.2b4)\n",
      "Requirement already satisfied: retrying in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from prometheus-api-client) (1.3.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib64/python3.7/site-packages (from prometheus-api-client) (0.24.1)\n",
      "Requirement already satisfied: dateparser in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from prometheus-api-client) (0.7.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.7/site-packages (from prometheus-api-client) (2.21.0)\n",
      "Requirement already satisfied: six>=1.7.0 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from retrying->prometheus-api-client) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from pandas->prometheus-api-client) (1.16.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/lib/python3.7/site-packages (from pandas->prometheus-api-client) (2018.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/lib/python3.7/site-packages (from pandas->prometheus-api-client) (2.8.0)\n",
      "Requirement already satisfied: tzlocal in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from dateparser->prometheus-api-client) (1.5.1)\n",
      "Requirement already satisfied: regex in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from dateparser->prometheus-api-client) (2019.3.12)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3.7/site-packages (from requests->prometheus-api-client) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3.7/site-packages (from requests->prometheus-api-client) (2.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/lib/python3.7/site-packages (from requests->prometheus-api-client) (1.24.2)\n",
      "Requirement already satisfied: pmdarima in /home/hemaveeradhi/.local/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.3 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from pmdarima) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from pmdarima) (0.14.0)\n",
      "Requirement already satisfied: Cython>=0.29 in /usr/local/lib64/python3.7/site-packages (from pmdarima) (0.29.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from pmdarima) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib64/python3.7/site-packages (from pmdarima) (0.20.3)\n",
      "Requirement already satisfied: statsmodels>=0.10.0 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from pmdarima) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from pmdarima) (1.16.1)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib64/python3.7/site-packages (from pmdarima) (0.24.1)\n",
      "Requirement already satisfied: patsy>=0.4.0 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from statsmodels>=0.10.0->pmdarima) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/lib/python3.7/site-packages (from pandas>=0.19->pmdarima) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/lib/python3.7/site-packages (from pandas>=0.19->pmdarima) (2018.5)\n",
      "Requirement already satisfied: matplotlib in /usr/lib64/python3.7/site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from matplotlib) (1.16.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib64/python3.7/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: six in /home/hemaveeradhi/.local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n"
     ]
    }
   ],
   "source": [
    "#We will be using the prometheus-api-client for formatting the metrics data and the pmdarima package for the ML model\n",
    "!pip3 install prometheus-api-client\n",
    "!pip3 install pmdarima\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the required modules/packages needed for all the fancy data science work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import json as JS\n",
    "from json import load\n",
    "from json import loads\n",
    "from datetime import datetime\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from prometheus_api_client import Metric, MetricsList, PrometheusConnect\n",
    "from prometheus_api_client.utils import parse_datetime, parse_timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using are Prometheus read/write metrics collected from a Dgraph application. Dgraph is an open-source, scalable, distributed, highly available and fast graph database, designed from the ground up to be run in production. The Dgraph instance provides metrics that follow Prometheus standards. One of the metrics are the disk metrics which track the disk activity of the Dgraph process. These metrics can be used for monitoring the read and write to persistent storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are loading the dataset that contains the past 30 days of disk write metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_json(\"metrics/prometheus-route-aiops-prod-prometheus-predict.cloud.paas.psi.redhat.com/badger_disk_writes:rate1m/1month of data (Aug 13-Sep 13)/201909131217.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'__name__': 'badger_disk_writes:rate1m', 'ae_...</td>\n",
       "      <td>[[1565799480.825, 0], [1565799540.825, 5.4], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'__name__': 'badger_disk_writes:rate1m', 'ae_...</td>\n",
       "      <td>[[1565799480.825, 0], [1565799540.825, 0], [15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'__name__': 'badger_disk_writes:rate1m', 'ae_...</td>\n",
       "      <td>[[1565799480.825, 0], [1565799540.825, 5.4], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'__name__': 'badger_disk_writes:rate1m', 'ae_...</td>\n",
       "      <td>[[1565799480.825, 0], [1565799540.825, 5.4], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'__name__': 'badger_disk_writes:rate1m', 'ae_...</td>\n",
       "      <td>[[1565799480.825, 0], [1565799540.825, 19.95],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              metric  \\\n",
       "0  {'__name__': 'badger_disk_writes:rate1m', 'ae_...   \n",
       "1  {'__name__': 'badger_disk_writes:rate1m', 'ae_...   \n",
       "2  {'__name__': 'badger_disk_writes:rate1m', 'ae_...   \n",
       "3  {'__name__': 'badger_disk_writes:rate1m', 'ae_...   \n",
       "4  {'__name__': 'badger_disk_writes:rate1m', 'ae_...   \n",
       "\n",
       "                                              values  \n",
       "0  [[1565799480.825, 0], [1565799540.825, 5.4], [...  \n",
       "1  [[1565799480.825, 0], [1565799540.825, 0], [15...  \n",
       "2  [[1565799480.825, 0], [1565799540.825, 5.4], [...  \n",
       "3  [[1565799480.825, 0], [1565799540.825, 5.4], [...  \n",
       "4  [[1565799480.825, 0], [1565799540.825, 19.95],...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the first few lines of the data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the structure of the `raw_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(type(raw_data))\n",
    "print(len(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: Try loading and viewing any of the `disk_read` metrics data instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data obtained from Prometheus is not very easy to understand nor is a suitable input for a machine learning model. To convert this raw data into an easier format we will make use of the Prometheus API Client.\n",
    "\n",
    "As we saw in the previous notebook, the Prometheus API client has a `MetricsList` object that creates a list of `Metric` objects, where each object is unique for a specific time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first convert the raw data into a `list` format as this is the input type required by the `MetricsList` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_data = raw_data.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metric_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can intitalize a `MetricsList` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_object_list = MetricsList(metric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "<class 'prometheus_api_client.metrics_list.MetricsList'>\n"
     ]
    }
   ],
   "source": [
    "print(len(metrics_object_list))\n",
    "print(type(metrics_object_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the items in this `MetricsList` object created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "metric_name: 'badger_disk_writes:rate1m'\n",
      "label_config: {'ae_source': 'http://prometheus-exporter-zero-0.thoth-dgraph-stage.svc:8080/debug/prometheus_metrics', 'instance': 'prometheus-aggregate-exporter-thoth-dgraph-stage.cloud.paas.psi.redhat.com:80', 'job': 'Thoth Dgraph aggregate-exporter (stage)', 'monitor': 'datahub'}\n",
      "metric_values:                                  ds       y\n",
      "0     2019-08-14 16:18:00.825000048   0.000\n",
      "1     2019-08-14 16:19:00.825000048   5.400\n",
      "2     2019-08-14 16:20:00.825000048   0.000\n",
      "3     2019-08-14 16:21:00.825000048   0.000\n",
      "4     2019-08-14 16:22:00.825000048   0.000\n",
      "5     2019-08-14 16:23:00.825000048   0.000\n",
      "6     2019-08-14 16:24:00.825000048   0.000\n",
      "7     2019-08-14 16:25:00.825000048   0.000\n",
      "8     2019-08-14 16:26:00.825000048   0.000\n",
      "9     2019-08-14 16:27:00.825000048   0.000\n",
      "10    2019-08-14 16:28:00.825000048   0.000\n",
      "11    2019-08-14 16:29:00.825000048   0.000\n",
      "12    2019-08-14 16:30:00.825000048   0.000\n",
      "13    2019-08-14 16:31:00.825000048   0.000\n",
      "14    2019-08-14 16:32:00.825000048   0.000\n",
      "15    2019-08-14 16:33:00.825000048   0.000\n",
      "16    2019-08-14 16:34:00.825000048   0.000\n",
      "17    2019-08-14 16:35:00.825000048   0.000\n",
      "18    2019-08-14 16:36:00.825000048   0.000\n",
      "19    2019-08-14 16:37:00.825000048   0.000\n",
      "20    2019-08-14 16:38:00.825000048   0.000\n",
      "21    2019-08-14 16:39:00.825000048   0.000\n",
      "22    2019-08-14 16:40:00.825000048   0.000\n",
      "23    2019-08-14 16:41:00.825000048   0.000\n",
      "24    2019-08-14 16:42:00.825000048   0.000\n",
      "25    2019-08-14 16:43:00.825000048   0.000\n",
      "26    2019-08-14 16:44:00.825000048   0.000\n",
      "27    2019-08-14 16:45:00.825000048   4.000\n",
      "28    2019-08-14 16:46:00.825000048  26.275\n",
      "29    2019-08-14 16:47:00.825000048  24.025\n",
      "...                             ...     ...\n",
      "42945 2019-09-13 15:48:00.825000048   0.000\n",
      "42946 2019-09-13 15:49:00.825000048   0.000\n",
      "42947 2019-09-13 15:50:00.825000048   0.000\n",
      "42948 2019-09-13 15:51:00.825000048   0.000\n",
      "42949 2019-09-13 15:52:00.825000048   0.000\n",
      "42950 2019-09-13 15:53:00.825000048   0.000\n",
      "42951 2019-09-13 15:54:00.825000048   0.000\n",
      "42952 2019-09-13 15:55:00.825000048   0.000\n",
      "42953 2019-09-13 15:56:00.825000048   0.000\n",
      "42954 2019-09-13 15:57:00.825000048   0.000\n",
      "42955 2019-09-13 15:58:00.825000048   0.000\n",
      "42956 2019-09-13 15:59:00.825000048   0.000\n",
      "42957 2019-09-13 16:00:00.825000048   0.000\n",
      "42958 2019-09-13 16:01:00.825000048   0.000\n",
      "42959 2019-09-13 16:02:00.825000048   0.000\n",
      "42960 2019-09-13 16:03:00.825000048   0.000\n",
      "42961 2019-09-13 16:04:00.825000048   0.000\n",
      "42962 2019-09-13 16:05:00.825000048   0.000\n",
      "42963 2019-09-13 16:06:00.825000048   0.000\n",
      "42964 2019-09-13 16:07:00.825000048   0.000\n",
      "42965 2019-09-13 16:08:00.825000048   0.000\n",
      "42966 2019-09-13 16:09:00.825000048   0.000\n",
      "42967 2019-09-13 16:10:00.825000048   0.000\n",
      "42968 2019-09-13 16:11:00.825000048   0.000\n",
      "42969 2019-09-13 16:12:00.825000048   0.000\n",
      "42970 2019-09-13 16:13:00.825000048   0.000\n",
      "42971 2019-09-13 16:14:00.825000048   0.000\n",
      "42972 2019-09-13 16:15:00.825000048   0.000\n",
      "42973 2019-09-13 16:16:00.825000048   0.000\n",
      "42974 2019-09-13 16:17:00.825000048   0.000\n",
      "\n",
      "[42975 rows x 2 columns]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "m = metrics_object_list[0]\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are only interested in the `metric_values` as this contains the timestamp and its corresponding metric value for which we want to train our machine learning model. So, lets extract only the metric values i.e the timestamp and its associated metric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                 ds       y\n",
      "0     2019-08-14 16:18:00.825000048   0.000\n",
      "1     2019-08-14 16:19:00.825000048   5.400\n",
      "2     2019-08-14 16:20:00.825000048   0.000\n",
      "3     2019-08-14 16:21:00.825000048   0.000\n",
      "4     2019-08-14 16:22:00.825000048   0.000\n",
      "5     2019-08-14 16:23:00.825000048   0.000\n",
      "6     2019-08-14 16:24:00.825000048   0.000\n",
      "7     2019-08-14 16:25:00.825000048   0.000\n",
      "8     2019-08-14 16:26:00.825000048   0.000\n",
      "9     2019-08-14 16:27:00.825000048   0.000\n",
      "10    2019-08-14 16:28:00.825000048   0.000\n",
      "11    2019-08-14 16:29:00.825000048   0.000\n",
      "12    2019-08-14 16:30:00.825000048   0.000\n",
      "13    2019-08-14 16:31:00.825000048   0.000\n",
      "14    2019-08-14 16:32:00.825000048   0.000\n",
      "15    2019-08-14 16:33:00.825000048   0.000\n",
      "16    2019-08-14 16:34:00.825000048   0.000\n",
      "17    2019-08-14 16:35:00.825000048   0.000\n",
      "18    2019-08-14 16:36:00.825000048   0.000\n",
      "19    2019-08-14 16:37:00.825000048   0.000\n",
      "20    2019-08-14 16:38:00.825000048   0.000\n",
      "21    2019-08-14 16:39:00.825000048   0.000\n",
      "22    2019-08-14 16:40:00.825000048   0.000\n",
      "23    2019-08-14 16:41:00.825000048   0.000\n",
      "24    2019-08-14 16:42:00.825000048   0.000\n",
      "25    2019-08-14 16:43:00.825000048   0.000\n",
      "26    2019-08-14 16:44:00.825000048   0.000\n",
      "27    2019-08-14 16:45:00.825000048   4.000\n",
      "28    2019-08-14 16:46:00.825000048  26.275\n",
      "29    2019-08-14 16:47:00.825000048  24.025\n",
      "...                             ...     ...\n",
      "42945 2019-09-13 15:48:00.825000048   0.000\n",
      "42946 2019-09-13 15:49:00.825000048   0.000\n",
      "42947 2019-09-13 15:50:00.825000048   0.000\n",
      "42948 2019-09-13 15:51:00.825000048   0.000\n",
      "42949 2019-09-13 15:52:00.825000048   0.000\n",
      "42950 2019-09-13 15:53:00.825000048   0.000\n",
      "42951 2019-09-13 15:54:00.825000048   0.000\n",
      "42952 2019-09-13 15:55:00.825000048   0.000\n",
      "42953 2019-09-13 15:56:00.825000048   0.000\n",
      "42954 2019-09-13 15:57:00.825000048   0.000\n",
      "42955 2019-09-13 15:58:00.825000048   0.000\n",
      "42956 2019-09-13 15:59:00.825000048   0.000\n",
      "42957 2019-09-13 16:00:00.825000048   0.000\n",
      "42958 2019-09-13 16:01:00.825000048   0.000\n",
      "42959 2019-09-13 16:02:00.825000048   0.000\n",
      "42960 2019-09-13 16:03:00.825000048   0.000\n",
      "42961 2019-09-13 16:04:00.825000048   0.000\n",
      "42962 2019-09-13 16:05:00.825000048   0.000\n",
      "42963 2019-09-13 16:06:00.825000048   0.000\n",
      "42964 2019-09-13 16:07:00.825000048   0.000\n",
      "42965 2019-09-13 16:08:00.825000048   0.000\n",
      "42966 2019-09-13 16:09:00.825000048   0.000\n",
      "42967 2019-09-13 16:10:00.825000048   0.000\n",
      "42968 2019-09-13 16:11:00.825000048   0.000\n",
      "42969 2019-09-13 16:12:00.825000048   0.000\n",
      "42970 2019-09-13 16:13:00.825000048   0.000\n",
      "42971 2019-09-13 16:14:00.825000048   0.000\n",
      "42972 2019-09-13 16:15:00.825000048   0.000\n",
      "42973 2019-09-13 16:16:00.825000048   0.000\n",
      "42974 2019-09-13 16:17:00.825000048   0.000\n",
      "\n",
      "[42975 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "m_values = m.metric_values\n",
    "print(type(m_values))\n",
    "print(m_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling\n",
    "As you can see, this data consists of metric values that are generated per **minute**, resulting in a large number of data points which may take a considerate amount of time when training the model. Hence, we will compress this data to a **hourly** frequency for the purpose of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1h' # Here we are setting the frequency to hourly, but we can also set this to be half-hourly or daily etc\n",
    "\n",
    "#Converting the dataframe to a single column with the timestamp column set as index\n",
    "m_values = m_values.set_index('ds')\n",
    "\n",
    "m_values = m_values.resample(freq).mean()\n",
    "m_values = m_values.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the compressed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721\n",
      "                             y\n",
      "ds                            \n",
      "2019-08-14 16:00:00   4.660627\n",
      "2019-08-14 17:00:00  16.956046\n",
      "2019-08-14 18:00:00  10.935411\n",
      "2019-08-14 19:00:00  10.280431\n",
      "2019-08-14 20:00:00   1.451667\n",
      "2019-08-14 21:00:00   0.684167\n",
      "2019-08-14 22:00:00   0.000000\n",
      "2019-08-14 23:00:00   1.218333\n",
      "2019-08-15 00:00:00  17.303737\n",
      "2019-08-15 01:00:00  18.721215\n",
      "2019-08-15 02:00:00  27.212932\n",
      "2019-08-15 03:00:00  21.176692\n",
      "2019-08-15 04:00:00  18.977896\n",
      "2019-08-15 05:00:00  16.302868\n",
      "2019-08-15 06:00:00  16.445853\n",
      "2019-08-15 07:00:00  25.498633\n",
      "2019-08-15 08:00:00  17.704142\n",
      "2019-08-15 09:00:00  19.119175\n",
      "2019-08-15 10:00:00  35.384672\n",
      "2019-08-15 11:00:00  27.904741\n",
      "2019-08-15 12:00:00   3.198761\n",
      "2019-08-15 13:00:00   3.035821\n",
      "2019-08-15 14:00:00   0.008333\n",
      "2019-08-15 15:00:00   8.582893\n",
      "2019-08-15 16:00:00   0.008333\n",
      "2019-08-15 17:00:00   0.712082\n",
      "2019-08-15 18:00:00   2.828306\n",
      "2019-08-15 19:00:00   0.135000\n",
      "2019-08-15 20:00:00   0.000000\n",
      "2019-08-15 21:00:00   0.000000\n",
      "...                        ...\n",
      "2019-09-12 11:00:00   0.000000\n",
      "2019-09-12 12:00:00   0.000000\n",
      "2019-09-12 13:00:00   0.000000\n",
      "2019-09-12 14:00:00   0.000000\n",
      "2019-09-12 15:00:00   0.000000\n",
      "2019-09-12 16:00:00   0.000000\n",
      "2019-09-12 17:00:00   0.000000\n",
      "2019-09-12 18:00:00   0.000000\n",
      "2019-09-12 19:00:00   0.000000\n",
      "2019-09-12 20:00:00   0.000000\n",
      "2019-09-12 21:00:00   0.000000\n",
      "2019-09-12 22:00:00   0.000000\n",
      "2019-09-12 23:00:00   0.000000\n",
      "2019-09-13 00:00:00  33.723333\n",
      "2019-09-13 01:00:00   6.644068\n",
      "2019-09-13 02:00:00   0.000000\n",
      "2019-09-13 03:00:00   0.000000\n",
      "2019-09-13 04:00:00   0.000000\n",
      "2019-09-13 05:00:00   0.000000\n",
      "2019-09-13 06:00:00   0.000000\n",
      "2019-09-13 07:00:00   0.000000\n",
      "2019-09-13 08:00:00   0.000000\n",
      "2019-09-13 09:00:00   0.000000\n",
      "2019-09-13 10:00:00   0.000000\n",
      "2019-09-13 11:00:00   0.000000\n",
      "2019-09-13 12:00:00   0.000000\n",
      "2019-09-13 13:00:00   0.000000\n",
      "2019-09-13 14:00:00   0.000000\n",
      "2019-09-13 15:00:00   0.000000\n",
      "2019-09-13 16:00:00   0.000000\n",
      "\n",
      "[721 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(len(m_values))\n",
    "print(m_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: How does the data look when we sample it differently? For example, instead of `hourly` try resampling the raw data to generate data of `daily` frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a much smaller dataset that we can look at. For training any machine learning model, we need to first split the data into `training` and `testing` datasets. \n",
    "\n",
    "* **Training** - The actual dataset that we use to train the model. The model sees and learns from this data\n",
    "* **Testing** - The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset\n",
    "\n",
    "Now that you know what these datasets do, we will split the dataset into Train and Test sets.\n",
    "\n",
    "[*As a rule of thumb, 80/20 is a good ratio to start with for splitting up the data into training and testing sets.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataframe):\n",
    "    #Dividing the data set into (80%)training and (20%)testing samples\n",
    "    dataframe = dataframe.sort_values(by = 'ds')\n",
    "    ratio = 0.2\n",
    "    size = int(len(dataframe) * (1-float(ratio)))\n",
    "    train, test = dataframe[0:size], dataframe[size:len(dataframe)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_data(m_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Data\n",
    "We are now ready to train a time seriers forecasting model. Here, we will train the SARIMA model i.e. Seasonal Auto-Regressive Moving Average model.\n",
    "\n",
    "### What is SARIMA?\n",
    "Autoregressive Integrated Moving Average, or ARIMA, is one of the most widely used forecasting methods for univariate time series data forecasting. Although this method can handle data with a trend, it does not support time series with a seasonal component i.e. time series with a repeating cycle. An extension to ARIMA that supports the direct modeling of the seasonal component of the series is called SARIMA.\n",
    "\n",
    "SARIMA adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n",
    "\n",
    "### What are hyperparameters?\n",
    "They are simply the very \"knobs\" one \"turns\" when building/tuning a statistical learning model. A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data. These hyperparametrs effect the speed and quality of the model training process. Different model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. \n",
    "\n",
    "We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.\n",
    "\n",
    "When a machine learning algorithm is tuned for a specific problem, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions.\n",
    "\n",
    "### How to configure SARIMA?\n",
    "Configuring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series. There are three **trend** elements that require configuration. These are the ones that are configured in the standard ARIMA model.\n",
    "\n",
    "* **p** - Trend autoregression order (AR part)\n",
    "\n",
    " - *It allows to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days.*\n",
    "\n",
    "* **d** - Trend difference order (I part)\n",
    "\n",
    " - *It is the number of nonseasonal differences needed for stationarity. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.*\n",
    "\n",
    "* **q** - Trend moving average order (MA part)\n",
    "\n",
    " - *It is the number of lagged forecast errors in the prediction equation. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.*\n",
    "\n",
    "There are four **seasonal** elements that are not part of ARIMA that must be configured; they are:\n",
    "\n",
    "* **P**: Seasonal autoregressive order.\n",
    "* **D**: Seasonal difference order.\n",
    "* **Q**: Seasonal moving average order.\n",
    "* **m**: The number of time steps for a single seasonal period.\n",
    "\n",
    "The Seasonal portion `(P, D, Q)m` has the same structure as the non-seasonal parts. It may — but does not have to include — an AR factor, an MA factor, and/or an I factor. In this part of the model, all of these factors operate across the number of period `(m)` in your season. This seasonality is a regular pattern of changes that repeats over `m` time periods. E.g. If there is a seasonal factor of 3 over a time series of monthly data, the pattern repeats every quarter.\n",
    "\n",
    "Put these all together and you get a `SARIMA (p, d, q) x (P, D, Q)m` which is what we’re looking to build.\n",
    "\n",
    "The exact model we’ll be using from the python package `statsmodels` is the `SARIMAX`, where `X` is the use of an exogenous explanatory variable (the X part of SARIMAX). We’re not going to get into adding exogenous variables to our model, so you can ignore this for now. The default will be `none` when we run the model as we don't have any extra variable to include at the moment.\n",
    "\n",
    "Together, the notation for a SARIMA model is specified as:\n",
    "\n",
    "```\n",
    "SARIMAX(order=(p,d,q), seasonal_order=(P,D,Q,m))\n",
    "```\n",
    "Source: http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
    "\n",
    "\n",
    "### Finding the hyperparameters\n",
    "In order to find the most optimized hyperparameters for the model and the given dataset, we carried out a Grid search method. Grid-searching is the process of scanning the data to configure optimal parameters for a given model. It is important to note that Grid-searching can be extremely computationally expensive and may take your machine quite a long time to run. Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination. We then choose the model/hyperparameters which resulted in the best accuracy or low error rates. \n",
    "\n",
    "A good explanation of how this is done can be found here: https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/\n",
    "\n",
    "Hence to save time and make things easier for you, we have already identified the optimized parameter values required to train the SARIMA model for this particular dataset. The values were found to be:\n",
    "* **p** = 1\n",
    "* **d** = 2\n",
    "* **q** = 2\n",
    "* **P** = 2\n",
    "* **D** = 2\n",
    "* **Q** = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to identify the relvant value for `m` which indicates the seasonality order for our data. Seasonality is the presence of any predictable fluctuation or pattern that recurs/repeats over regular intervals. For example, data with daily observations might have a weekly seasonality (frequency=7) or an annual seasonality (frequency=365.25). Similarly, data that are observed every minute might have an hourly seasonality (frequency=60), a daily seasonality (frequency=24x60=1440), a weekly seasonality (frequency=24x60x7=10080) and an annual seasonality (frequency=24x60x365.25=525960).\n",
    "\n",
    "The raw data obtained from Prometheus for the disk write metric was observed every minute and had a **daily** seasonality. So, depending on how you resample your data i.e. whether it is hourly,minutely,daily etc we can determine the corresponding daily seasonality order. For example, if the data is sampled to be `hourly` then `m = 24 (i.e. 1 day = 24 hours)`. \n",
    "\n",
    "You can read more about finding the seasonal periods here: https://robjhyndman.com/hyndsight/seasonal-periods/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the `_sarima()` function which performs the following tasks:\n",
    "    * Training the model\n",
    "    * Fitting the model\n",
    "    * Interpretting the model forecast\n",
    "    * Plotting the forecasted values\n",
    "    * Plotting the forecasted vs actual/test data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sarima(train, test, freq):\n",
    "    #Determining the seasonal order for the corresponding frequency of data\n",
    "    sfrequency = 0\n",
    "    if freq == '1h':\n",
    "        sfrequency = 24\n",
    "    elif freq == '30Min':\n",
    "        sfrequency = 48\n",
    "    elif freq == '15Min':\n",
    "        sfrequency = 96\n",
    "    elif freq == '1d':\n",
    "        sfrequency = 1\n",
    "        \n",
    "    model = SARIMAX(train, order=(1, 2, 2), seasonal_order=(2,2,2,sfrequency), enforce_stationarity = True, enforce_invertibility = False)\n",
    "    model_fit = model.fit(dsip=-1)\n",
    "    residuals = pd.DataFrame(model_fit.resid)\n",
    "    residuals.plot()\n",
    "    print(\"Now plotting the residual errors for the SARIMA model\")\n",
    "    plt.show()\n",
    "    residuals.plot(kind='kde')\n",
    "    print(\"Now plotting the desity of the residual error values.\")\n",
    "    plt.show()\n",
    "    print(residuals.describe())\n",
    "    K = len(test)\n",
    "\n",
    "    forecast = model_fit.forecast(K)\n",
    "    forecast = pd.DataFrame(forecast.values, columns = ['predict'], index = test.index)\n",
    "\n",
    "    plt.plot(forecast, label='forecast')\n",
    "    print(\"Now plotting the forecasted values.\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12,5), dpi=100)\n",
    "    plt.plot(test, label='actual')\n",
    "    plt.plot(forecast, label='forecast')\n",
    "    plt.legend(loc='upper left', fontsize=8)\n",
    "    print(\"Now plotting the forecasted vs the test data.\")\n",
    "    plt.show()\n",
    "    return model, model_fit, forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our SARIMA model function is defined, we can train the model with our dataset by specifying the frequency of the resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we specify the frequency of our data i.e. whether it is minutely/hourly/daily etc data. If you resampled your data differently, remember to indicate this change here as well\n",
    "freq = '1h'\n",
    "model, model_fit, forecast = _sarima(train, test, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretting the Model Forecast\n",
    "#### Residuals\n",
    "The “residuals” in a time series model are what is left over after fitting a model. For many (but not all) time series models, the residuals are equal to the difference between the observations and the corresponding fitted values. Residuals are useful in checking whether a model has adequately captured the information in the data. \n",
    "\n",
    "Generally, when exploring residual errors we are looking for patterns or structure. A sign of a pattern suggests that the errors are not random.\n",
    "\n",
    "We expect the residual errors to be random, because it means that the model has captured all of the structure and the only error left is the random fluctuations in the time series that cannot be modeled.\n",
    "\n",
    "A sign of a pattern or structure suggests that there is more information that a model could capture and use to make better predictions.\n",
    "\n",
    "#### Residual Summary Statistics\n",
    "We can also calculate summary statistics on the residual errors to understand their distribution. This includes the mean and standard deviation of the distribution, as well as percentiles and the minimum and maximum errors observed.\n",
    "\n",
    "Primarily, we are interested in the mean value of the residual errors. A value close to zero suggests no bias in the forecasts, whereas positive and negative values suggest a positive or negative bias in the forecasts made. It is useful to know about a bias in the forecasts as it can be directly corrected in forecasts prior to their use or evaluation. Plots can be used to better understand the distribution of errors beyond summary statistics. We can use either histograms or density plots to better understand the distribution of residual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** - Try changing the hyperparameter values (p,d,q,P,D,Q,m) and see how this affects the model's performance in detecting anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4** - Try to interpret the residual summary statistics obtained. Do these results suggest that the model can be further improved? \n",
    "\n",
    "[Hint: Observe whether the mean value is 0, positive or negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that the model has successfully completed training, let's identify anomalies by comparing the `predicted` values with the `actual` values (test dataset). To do so, we will define `upper` and `lower` bounds based on the range of predictions made by the model. If the actual metric value does not lie within these defined bounds, we will flag that as an anomaly.\n",
    "\n",
    "* **Upper bound** - Calculated as the **sum** of the predicted value and (2x its standard deviation)\n",
    "* **Lower bound** - Calculated as the **difference** between the predicted value and (2x its standard deviation)\n",
    "* **Anomaly** - Boolean value where 1 is an anomaly and 0 is not an anomaly\n",
    "    - If, (actual value < lower bound) **or** (actual value > upper bound) then anomaly = 1\n",
    "    - Else, anomaly = 0\n",
    "    \n",
    "We can plot a graph of the **forecasted values, actual values, upper bound, lower bound and the anomaly detected** to understand how the machine learning model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(forecast, test):\n",
    "    #Calculating the upper and lower bounds\n",
    "    upper_bound = np.array(\n",
    "      [\n",
    "        (\n",
    "         forecast['predict'][i]\n",
    "          + (np.std(forecast[:i])*2)\n",
    "        )\n",
    "        for i in range(len(forecast))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lower_bound = np.array(\n",
    "      [\n",
    "        (\n",
    "          forecast['predict'][i]\n",
    "          - (np.std(forecast[:i])*2)\n",
    "        )\n",
    "        for i in range(len(forecast))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    compare = test.join(forecast, how = 'outer')\n",
    "    compare = compare.dropna()\n",
    "    lower = pd.DataFrame(lower_bound, columns = [0], index = test.index)\n",
    "    upper = pd.DataFrame(upper_bound, columns = [0], index = test.index)\n",
    "\n",
    "    compare.insert(2, \"LowerBound\", lower, True)\n",
    "    compare.insert(3, \"UpperBound\", upper, True)\n",
    "    compare['WithinUpperBounds'] = compare['predict'] < compare['UpperBound']\n",
    "    compare['WithinLowerBounds'] = compare['predict'] > compare['LowerBound']\n",
    "    \n",
    "    #Detecting anomalies\n",
    "    anomaly = []\n",
    "    for i in range(len(compare)):\n",
    "        if((compare['y'][i] < compare['UpperBound'][i]) and (compare['y'][i] > compare['LowerBound'][i])):\n",
    "            anomaly.append(0)\n",
    "        else:\n",
    "            anomaly.append(1)\n",
    "    \n",
    "    compare.insert(6, \"Anomaly\", anomaly, True)\n",
    "    print(\"The final dataframe with lowerbound, upperbound and prediction value:\\n\")\n",
    "    print(compare)\n",
    "\n",
    "    plt.figure(figsize=(12,5), dpi=100)\n",
    "    plt.plot(compare['predict'], label='Predicted')\n",
    "    plt.plot(compare['LowerBound'], label='LowerBound')\n",
    "    plt.plot(compare['UpperBound'], label='UpperBound')\n",
    "    plt.plot(compare['y'], label='Actual')\n",
    "    plt.legend(loc='upper left', fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets evaluate the forecasted values with the `test` data set to identify anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(forecast, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5** - Now, train the model with `disk_reads` metrics. Try looking at different datasets (30 days vs 90 days) and see how the model performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you enjoyed this workshop and learnt some fun stuff on leveraging AI/ML for detecting anomalies using Prometheus. Know any other interesting model that can perform time series forecasting? We would be more than happy to have your contributions towards improving this project! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
